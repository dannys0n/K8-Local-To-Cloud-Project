{
  "Makefile": "KIND_CLUSTER ?= dev\nCLIENTS ?= 50\n\n# To get running:\n# make up\n# make game-testbed\n# make proxy-port-forward-local\n# game-load-local CLIENTS=100\n\nup: cluster monitors managers databases port-forward\n\ncluster:\n\t./src/scripts/cluster.sh\n\nmonitors:\n\t./src/scripts/monitors.sh\n\nmanagers:\n\t./src/scripts/managers.sh\n\ndatabases: redis postgres\n\ndatabases-namespace:\n\tkubectl apply -f src/databases/namespace.yaml\n\nredis: databases-namespace\n\tkubectl apply -f src/databases/redis.yaml\n\tkubectl rollout status deployment/redis -n databases\n\npostgres: databases-namespace\n\tkubectl apply -f src/databases/postgres.yaml\n\tkubectl rollout status deployment/postgres -n databases\n\nport-forward:\n\t./src/scripts/port-forward.sh\n\n\ndown:\n\t./src/scripts/teardown.sh\n\nstatus:\n\tkubectl get pods -n monitoring\n\n# --- Database cleanups\n\n# run if problems with database ports\nfree-local-db-ports:\n\tbash ./src/scripts/free-local-db-ports.sh\n\nclear-databases:\n\tbash ./src/scripts/clear-databases.sh\n\n# --- Game backend testbed:\n# 1\ngame-testbed: game-build game-load-images game-backend game-proxy\n# 2\nproxy-port-forward-local:\n\tkubectl port-forward svc/game-proxy 8080:8080\n# 3\ngame-load-local:\n\tcd src/app/load && python3 main.py $(CLIENTS)\n\ngame-build:\n\tdocker build -t game-backend:local src/app/backend\n\tdocker build -t game-proxy:local  src/app/proxy\n\tdocker build -t game-server:local  src/app/game-server\n\ngame-load-images:\n\tkind load docker-image game-backend:local --name $(KIND_CLUSTER)\n\tkind load docker-image game-proxy:local  --name $(KIND_CLUSTER)\n\tkind load docker-image game-server:local --name $(KIND_CLUSTER)\n\ngame-backend:\n\tkubectl apply -f src/k8s/base/backend-rbac.yaml\n\tkubectl apply -f src/k8s/base/backend.yaml\n\tkubectl apply -f src/k8s/base/backend-hpa.yaml\n\ngame-proxy:\n\tkubectl apply -f src/k8s/base/proxy.yaml\n\tkubectl apply -f src/k8s/base/proxy-hpa.yaml\n\nping-redis:\n\t./src/scripts/ping-redis.sh\n\nping-postgres:\n\t./src/scripts/ping-postgres.sh\n",
  "src/kind/cluster.yaml": "kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n  - role: control-plane\n",
  "src/app/proxy/main.py": "\"\"\"Game proxy: single entry point for clients.\"\"\"\nfrom fastapi import FastAPI, Request, HTTPException\nfrom fastapi.responses import JSONResponse\n\nfrom backend_client import forward_json, health_backend, shutdown_client, startup_client\n\n\napp = FastAPI(title=\"Game Proxy\", version=\"0.1.0\")\n\n\n@app.on_event(\"startup\")\nasync def startup() -> None:\n    await startup_client()\n\n\n@app.on_event(\"shutdown\")\nasync def shutdown() -> None:\n    await shutdown_client()\n\n\n@app.get(\"/health\")\nasync def health() -> dict:\n    return await health_backend()\n\n\n@app.exception_handler(Exception)\nasync def global_exception_handler(request: Request, exc: Exception):\n    \"\"\"Catch all exceptions to prevent proxy crashes.\"\"\"\n    import traceback\n    print(f\"Unhandled exception in proxy: {exc}\\n{traceback.format_exc()}\")\n    return JSONResponse(\n        status_code=500,\n        content={\"error\": \"Internal proxy error\", \"detail\": str(exc)},\n    )\n\n\n@app.post(\"/api/match/join\")\nasync def proxy_join(request: Request):\n    \"\"\"\n    Thin HTTP proxy in front of the backend.\n    Forwards JSON body to /match/join on the backend service.\n    \"\"\"\n    body = await request.body()\n    content_type = request.headers.get(\"content-type\", \"application/json\")\n    return await forward_json(\n        \"POST\",\n        \"/match/join\",\n        content=body,\n        headers={\"content-type\": content_type},\n        timeout=10.0,\n    )\n\n\n@app.post(\"/api/match/{session_id}/end\")\nasync def proxy_end(session_id: str, request: Request):\n    \"\"\"Forward /match/{session_id}/end to backend.\"\"\"\n    return await forward_json(\n        \"POST\",\n        f\"/match/{session_id}/end\",\n        timeout=15.0,\n    )\n\n\n@app.get(\"/api/match/status\")\nasync def proxy_match_status(player_id: str):\n    \"\"\"Forward status so queued clients can poll until matched and get game server address.\"\"\"\n    return await forward_json(\n        \"GET\",\n        \"/match/status\",\n        params={\"player_id\": player_id},\n        timeout=5.0,\n    )\n\n\n@app.get(\"/api/sessions/active\")\nasync def proxy_active_sessions():\n    \"\"\"Forward /sessions/active to backend.\"\"\"\n    return await forward_json(\"GET\", \"/sessions/active\", timeout=5.0)\n\n",
  "src/app/proxy/backend_client.py": "import os\n\nimport httpx\nfrom fastapi import HTTPException\n\n\nBACKEND_URL = os.getenv(\"BACKEND_URL\", \"http://game-backend:8080\")\n_client: httpx.AsyncClient | None = None\n\n\nasync def startup_client() -> None:\n    global _client\n    limits = httpx.Limits(max_keepalive_connections=100, max_connections=200)\n    _client = httpx.AsyncClient(\n        base_url=BACKEND_URL,\n        timeout=30.0,\n        limits=limits,\n    )\n\n\nasync def shutdown_client() -> None:\n    global _client\n    if _client is not None:\n        await _client.aclose()\n        _client = None\n\n\nasync def health_backend() -> dict:\n    if _client is None:\n        return {\"status\": \"degraded\", \"reason\": \"client_not_initialized\"}\n    try:\n        resp = await _client.get(\"/health\", timeout=2.0)\n        if resp.status_code == 200:\n            return {\"status\": \"ok\"}\n        return {\"status\": \"degraded\", \"reason\": f\"backend_status_{resp.status_code}\"}\n    except Exception:  # noqa: BLE001\n        return {\"status\": \"degraded\", \"reason\": \"backend_unreachable\"}\n\n\nasync def forward_json(\n    method: str,\n    path: str,\n    *,\n    content: bytes | None = None,\n    params: dict | None = None,\n    headers: dict | None = None,\n    timeout: float = 10.0,\n):\n    if _client is None:\n        raise HTTPException(status_code=503, detail=\"Proxy not ready\")\n    try:\n        if method == \"GET\":\n            resp = await _client.get(path, params=params, timeout=timeout, headers=headers)\n        elif method == \"POST\":\n            resp = await _client.post(path, content=content, timeout=timeout, headers=headers)\n        else:\n            raise HTTPException(status_code=500, detail=f\"Unsupported method: {method}\")\n        resp.raise_for_status()\n        return resp.json()\n    except httpx.TimeoutException:\n        raise HTTPException(status_code=504, detail=\"Backend timeout\")\n    except httpx.RequestError as e:\n        raise HTTPException(status_code=502, detail=f\"Backend connection error: {str(e)}\")\n    except HTTPException:\n        raise\n    except Exception as e:  # noqa: BLE001\n        raise HTTPException(status_code=500, detail=f\"Proxy error: {str(e)}\")\n",
  "src/app/proxy/Dockerfile": "FROM python:3.12-slim\n\nWORKDIR /app\n\nRUN pip install --no-cache-dir fastapi uvicorn[standard] httpx\n\nCOPY *.py .\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n\n",
  "src/app/game-server/main.py": "\"\"\"\nGame server: TCP server with authoritative state (open -> running -> stop).\nMatch config (e.g. duration) is decided by clients (custom match); server notifies\nclients on state changes and closes itself when done or when no clients remain in running.\n\"\"\"\nimport asyncio\nimport os\nimport sys\nimport time\n\nSESSION_ID = os.getenv(\"SESSION_ID\", \"unknown\")\nPORT = int(os.getenv(\"PORT\", \"8080\"))\n\n# State: open -> running -> stop (server-authoritative)\n_state = \"open\"\n# Match duration in seconds; set by first client via REQUEST_MATCH <sec>\n_match_duration_seconds: float | None = None\n# When we transitioned to running (time.time())\n_running_started_at: float | None = None\n_clients: list[asyncio.StreamWriter] = []\n_state_lock = asyncio.Lock()\n_shutdown = asyncio.Event()\n\n\ndef _get_state() -> str:\n    return _state\n\n\ndef _set_state(new: str) -> None:\n    global _state\n    _state = new\n\n\nasync def _broadcast(line: str) -> None:\n    \"\"\"Send a line to all connected clients (with newline).\"\"\"\n    msg = (line if line.endswith(\"\\n\") else line + \"\\n\").encode()\n    async with _state_lock:\n        for w in _clients:\n            try:\n                w.write(msg)\n                await w.drain()\n            except Exception:  # noqa: BLE001\n                pass\n\n\nasync def _run_timer(duration_seconds: float) -> None:\n    \"\"\"After duration_seconds in 'running', transition to stop and trigger shutdown.\"\"\"\n    await asyncio.sleep(duration_seconds)\n    async with _state_lock:\n        if _get_state() != \"running\":\n            return\n        _set_state(\"stop\")\n    await _broadcast(\"STATE stop\")\n    _shutdown.set()\n\n\ndef _check_empty_and_stop() -> None:\n    \"\"\"If in running state and no clients left, close session early.\"\"\"\n    global _state\n    if _state == \"running\" and len(_clients) == 0:\n        _set_state(\"stop\")\n        _shutdown.set()\n\n\nasync def _handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> None:\n    global _match_duration_seconds\n    async with _state_lock:\n        _clients.append(writer)\n\n    try:\n        while True:\n            line = await reader.readline()\n            if not line:\n                break\n            raw = line.decode().strip()\n            cmd = raw.upper()\n            if cmd == \"GET_STATE\":\n                writer.write(f\"STATE {_get_state()}\\n\".encode())\n                await writer.drain()\n            elif cmd == \"GET_RUNNING_LENGTH\":\n                dur = _match_duration_seconds\n                if dur is not None:\n                    writer.write(f\"RUNNING_LENGTH {int(dur)}\\n\".encode())\n                else:\n                    writer.write(b\"RUNNING_LENGTH 0\\n\")\n                await writer.drain()\n            elif cmd.startswith(\"REQUEST_MATCH \"):\n                # Client requests custom match duration (seconds). First one wins.\n                parts = raw.split()\n                if len(parts) == 2:\n                    try:\n                        sec = float(parts[1])\n                        if sec > 0 and sec <= 86400:  # cap 24h\n                            started_now = False\n                            async with _state_lock:\n                                if _match_duration_seconds is None and _get_state() == \"open\":\n                                    _match_duration_seconds = sec\n                                    _set_state(\"running\")\n                                    global _running_started_at\n                                    _running_started_at = time.time()\n                                    asyncio.create_task(_run_timer(sec))\n                                    started_now = True\n                            writer.write(f\"RUNNING_LENGTH {int(_match_duration_seconds or 0)}\\n\".encode())\n                            await writer.drain()\n                            if started_now:\n                                await _broadcast(\"STATE running\")\n                            continue\n                    except ValueError:\n                        pass\n                writer.write(b\"UNKNOWN\\n\")\n                await writer.drain()\n            else:\n                writer.write(b\"UNKNOWN\\n\")\n                await writer.drain()\n    except (ConnectionResetError, BrokenPipeError, asyncio.CancelledError):\n        pass\n    finally:\n        async with _state_lock:\n            if writer in _clients:\n                _clients.remove(writer)\n            _check_empty_and_stop()\n        try:\n            writer.close()\n            await writer.wait_closed()\n        except Exception:  # noqa: BLE001\n            pass\n\n\nasync def _serve() -> None:\n    server = await asyncio.start_server(_handle_client, \"0.0.0.0\", PORT)\n    async with server:\n        await _shutdown.wait()\n    server.close()\n    await server.wait_closed()\n    # Close all client connections before exiting\n    async with _state_lock:\n        for w in _clients[:]:\n            try:\n                w.close()\n                await w.wait_closed()\n            except Exception:  # noqa: BLE001\n                pass\n            _clients.clear()\n\n\ndef main() -> int:\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    try:\n        loop.run_until_complete(_serve())\n        return 0\n    except KeyboardInterrupt:\n        return 0\n    finally:\n        loop.close()\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "src/app/game-server/Dockerfile": "FROM python:3.12-slim\n\nWORKDIR /app\n\nCOPY main.py .\n\nCMD [\"python\", \"main.py\"]\n",
  "src/app/backend/settings.py": "import os\n\n\nSESSION_SIZE = int(os.getenv(\"SESSION_SIZE\", \"12\"))\nFLUSH_WAIT_SECONDS = float(os.getenv(\"FLUSH_WAIT_SECONDS\", \"15\"))\nMIN_PARTIAL_SESSION_SIZE = int(os.getenv(\"MIN_PARTIAL_SESSION_SIZE\", \"2\"))\n\nDATABASE_URL = os.getenv(\n    \"DATABASE_URL\",\n    \"postgresql://postgres:postgres@postgres.databases.svc.cluster.local:5432/app\",\n)\nREDIS_HOST = os.getenv(\"REDIS_HOST\", \"redis.databases.svc.cluster.local\")\nREDIS_PORT = int(os.getenv(\"REDIS_PORT\", \"6379\"))\nNAMESPACE = os.getenv(\"NAMESPACE\", \"default\")\n",
  "src/app/backend/main.py": "from typing import List\nimport logging\nimport os\nimport time\nimport uuid\n\nfrom fastapi import FastAPI, HTTPException\nfrom kubernetes import client\nfrom kubernetes.client.rest import ApiException\n\nfrom k8s_game_server import create_game_server_pod, delete_game_server_pod, get_core_v1_api, get_k8s_api\nfrom models import MatchRequest, MatchResponse\nfrom settings import FLUSH_WAIT_SECONDS, MIN_PARTIAL_SESSION_SIZE, NAMESPACE, SESSION_SIZE\nfrom storage import (\n    append_local_queue,\n    dequeue_players,\n    get_db_conn,\n    get_player_queue_ts_key,\n    get_redis_client,\n    local_dequeue,\n    local_oldest_wait_seconds,\n    local_queue_len,\n    track_session_in_redis,\n    untrack_session_in_redis,\n)\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\napp = FastAPI(title=\"Game Backend\", version=\"0.1.0\")\n\n\ndef _create_match_session(players: List[str]) -> MatchResponse:\n    session_id = str(uuid.uuid4())\n    conn = get_db_conn()\n    backend_pod = os.getenv(\"HOSTNAME\", \"unknown\")\n    players_json = \",\".join(players)\n\n    with conn.cursor() as cur:\n        cur.execute(\n            \"INSERT INTO matches (session_id, players_json, backend_pod) VALUES (%s, %s, %s)\",\n            (session_id, players_json, backend_pod),\n        )\n\n    game_server_pod, connect_host, connect_port = create_game_server_pod(session_id, players)\n    with conn.cursor() as cur:\n        cur.execute(\n            \"UPDATE matches SET game_server_pod = %s WHERE session_id = %s\",\n            (game_server_pod, session_id),\n        )\n    track_session_in_redis(session_id, game_server_pod, connect_host, connect_port)\n    return MatchResponse(\n        session_id=session_id,\n        players=players,\n        connect_host=connect_host,\n        connect_port=connect_port,\n    )\n\n\n@app.get(\"/health\")\ndef health() -> dict:\n    try:\n        conn = get_db_conn()\n        with conn.cursor() as cur:\n            cur.execute(\"SELECT 1;\")\n    except Exception:  # noqa: BLE001\n        return {\"status\": \"degraded\"}\n    return {\"status\": \"ok\"}\n\n\n@app.post(\"/match/join\", response_model=MatchResponse)\ndef join_match(req: MatchRequest) -> MatchResponse:\n    redis_client = get_redis_client()\n    queue_key = \"matchmaking_queue\"\n\n    if redis_client:\n        try:\n            redis_client.rpush(queue_key, req.player_id)\n            redis_client.set(get_player_queue_ts_key(req.player_id), str(time.time()), ex=3600)\n            queue_len = redis_client.llen(queue_key)\n\n            flush_count = 0\n            if queue_len >= SESSION_SIZE:\n                flush_count = SESSION_SIZE\n            elif queue_len >= MIN_PARTIAL_SESSION_SIZE:\n                oldest_player = redis_client.lindex(queue_key, 0)\n                if oldest_player:\n                    queued_at_raw = redis_client.get(get_player_queue_ts_key(oldest_player))\n                    if queued_at_raw:\n                        try:\n                            oldest_wait = time.time() - float(queued_at_raw)\n                            if oldest_wait >= FLUSH_WAIT_SECONDS:\n                                flush_count = queue_len\n                        except ValueError:\n                            pass\n\n            if flush_count > 0:\n                players = dequeue_players(redis_client, queue_key, flush_count)\n                if len(players) == flush_count:\n                    return _create_match_session(players)\n        except Exception as e:  # noqa: BLE001\n            logger.warning(f\"Redis queue operation failed: {e}, falling back to in-memory\")\n\n    append_local_queue(req.player_id)\n    local_len = local_queue_len()\n    local_flush_count = 0\n    if local_len >= SESSION_SIZE:\n        local_flush_count = SESSION_SIZE\n    elif local_len >= MIN_PARTIAL_SESSION_SIZE:\n        if local_oldest_wait_seconds() >= FLUSH_WAIT_SECONDS:\n            local_flush_count = local_len\n\n    if local_flush_count > 0:\n        players = local_dequeue(local_flush_count)\n        return _create_match_session(players)\n\n    return MatchResponse(session_id=f\"pending:{req.player_id}\", players=[req.player_id])\n\n\n@app.get(\"/match/status\")\ndef match_status(player_id: str) -> dict:\n    conn = get_db_conn()\n    with conn.cursor() as cur:\n        cur.execute(\n            \"\"\"\n            SELECT session_id, ended_at\n            FROM matches\n            WHERE\n              players_json = %s\n              OR players_json LIKE %s\n              OR players_json LIKE %s\n              OR players_json LIKE %s\n            ORDER BY created_at DESC\n            LIMIT 1\n            \"\"\",\n            (\n                player_id,\n                f\"{player_id},%\",\n                f\"%,{player_id},%\",\n                f\"%,{player_id}\",\n            ),\n        )\n        row = cur.fetchone()\n\n    if not row:\n        return {\"status\": \"pending\"}\n\n    session_id, ended_at = row\n    if ended_at:\n        return {\"status\": \"ended\", \"session_id\": session_id}\n\n    redis_client = get_redis_client()\n    if redis_client:\n        host = redis_client.get(f\"session:{session_id}:host\") or \"\"\n        port_str = redis_client.get(f\"session:{session_id}:port\") or \"0\"\n        port = int(port_str) if port_str.isdigit() else 0\n        if host and port:\n            return {\n                \"status\": \"matched\",\n                \"session_id\": session_id,\n                \"connect_host\": host,\n                \"connect_port\": port,\n            }\n    return {\"status\": \"pending\"}\n\n\n@app.post(\"/match/{session_id}/end\")\ndef end_match(session_id: str) -> dict:\n    try:\n        delete_game_server_pod(session_id)\n    except Exception as e:  # noqa: BLE001\n        logger.error(f\"Failed to delete game server pod for {session_id}: {e}\")\n\n    conn = get_db_conn()\n    with conn.cursor() as cur:\n        cur.execute(\n            \"UPDATE matches SET ended_at = now() WHERE session_id = %s\",\n            (session_id,),\n        )\n    untrack_session_in_redis(session_id)\n    return {\"status\": \"ended\", \"session_id\": session_id}\n\n\n@app.get(\"/sessions/active\")\ndef get_active_sessions() -> dict:\n    redis_client = get_redis_client()\n    if redis_client:\n        try:\n            session_ids = redis_client.smembers(\"active_sessions\")\n            sessions = []\n            for sid in session_ids:\n                pod = redis_client.get(f\"session:{sid}:pod\") or \"unknown\"\n                sessions.append({\"session_id\": sid, \"game_server_pod\": pod})\n            return {\n                \"count\": len(session_ids),\n                \"sessions\": sessions,\n                \"source\": \"redis\",\n                \"note\": \"Each session = 1 match with 12 players\",\n            }\n        except Exception as e:  # noqa: BLE001\n            logger.warning(f\"Redis query failed: {e}, falling back to Postgres\")\n\n    try:\n        conn = get_db_conn()\n        with conn.cursor() as cur:\n            cur.execute(\n                \"\"\"\n                SELECT session_id, game_server_pod, created_at\n                FROM matches\n                WHERE ended_at IS NULL\n                  AND created_at > now() - interval '5 minutes'\n                ORDER BY created_at DESC\n                \"\"\"\n            )\n            rows = cur.fetchall()\n            sessions = [\n                {\n                    \"session_id\": row[0],\n                    \"game_server_pod\": row[1] or \"unknown\",\n                    \"created_at\": str(row[2]) if row[2] else None,\n                }\n                for row in rows\n            ]\n            return {\n                \"count\": len(sessions),\n                \"sessions\": sessions,\n                \"source\": \"postgres\",\n                \"note\": \"Each session = 1 match with 12 players (showing last 5 minutes only)\",\n            }\n    except Exception as e:  # noqa: BLE001\n        logger.error(f\"Failed to query active sessions: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Failed to get active sessions: {e}\")\n\n\n@app.post(\"/cleanup/orphaned-servers\")\ndef cleanup_orphaned_servers() -> dict:\n    conn = get_db_conn()\n    k8s_apps_api = get_k8s_api()\n    try:\n        deployments = k8s_apps_api.list_namespaced_deployment(\n            namespace=NAMESPACE,\n            label_selector=\"app=game-server\",\n        )\n    except Exception as e:  # noqa: BLE001\n        logger.error(f\"Failed to list deployments: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Failed to list deployments: {e}\")\n\n    cleaned = 0\n    with conn.cursor() as cur:\n        for dep in deployments.items:\n            dep_name = dep.metadata.name\n            if not dep_name.startswith(\"game-server-\"):\n                continue\n            session_prefix = dep_name.replace(\"game-server-\", \"\")\n            cur.execute(\n                \"SELECT ended_at FROM matches WHERE session_id LIKE %s\",\n                (f\"{session_prefix}%\",),\n            )\n            row = cur.fetchone()\n            if row and row[0]:\n                try:\n                    core_api = get_core_v1_api()\n                    try:\n                        core_api.delete_namespaced_service(name=dep_name, namespace=NAMESPACE)\n                    except ApiException:  # noqa: BLE001\n                        pass\n                    k8s_apps_api.delete_namespaced_deployment(\n                        name=dep_name,\n                        namespace=NAMESPACE,\n                        body=client.V1DeleteOptions(propagation_policy=\"Foreground\"),\n                    )\n                    cleaned += 1\n                    logger.info(f\"Cleaned up orphaned deployment: {dep_name}\")\n                except ApiException as e:\n                    if e.status != 404:\n                        logger.warning(f\"Failed to delete {dep_name}: {e}\")\n\n    return {\"cleaned\": cleaned, \"message\": f\"Cleaned up {cleaned} orphaned game server deployments\"}\n\n\n",
  "src/app/backend/models.py": "from typing import List\n\nfrom pydantic import BaseModel\n\n\nclass MatchRequest(BaseModel):\n    player_id: str\n\n\nclass MatchResponse(BaseModel):\n    session_id: str\n    players: List[str]\n    connect_host: str = \"\"\n    connect_port: int = 0\n",
  "src/app/backend/storage.py": "from collections import deque\nimport logging\nimport os\nimport time\nfrom typing import List\n\nimport psycopg2\nimport redis\n\nfrom settings import DATABASE_URL, REDIS_HOST, REDIS_PORT\n\nlogger = logging.getLogger(__name__)\n\n_db_conn = None\n_redis_client = None\n_local_queue = deque()\n\n\ndef get_db_conn():\n    global _db_conn\n    if _db_conn is None:\n        _db_conn = psycopg2.connect(DATABASE_URL)\n        _db_conn.autocommit = True\n        ensure_schema(_db_conn)\n    return _db_conn\n\n\ndef get_redis_client():\n    global _redis_client\n    if _redis_client is None:\n        try:\n            _redis_client = redis.Redis(\n                host=REDIS_HOST,\n                port=REDIS_PORT,\n                decode_responses=True,\n                socket_connect_timeout=2,\n            )\n            _redis_client.ping()\n            logger.info(f\"Connected to Redis at {REDIS_HOST}:{REDIS_PORT}\")\n        except Exception as e:  # noqa: BLE001\n            logger.warning(f\"Redis connection failed: {e}, continuing without Redis\")\n            _redis_client = None\n    return _redis_client\n\n\ndef ensure_schema(conn) -> None:\n    with conn.cursor() as cur:\n        cur.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS matches (\n              session_id text PRIMARY KEY,\n              players_json text NOT NULL,\n              backend_pod text,\n              created_at timestamptz DEFAULT now()\n            );\n            \"\"\"\n        )\n        cur.execute(\n            \"\"\"\n            SELECT column_name\n            FROM information_schema.columns\n            WHERE table_name='matches' AND column_name='game_server_pod';\n            \"\"\"\n        )\n        if not cur.fetchone():\n            cur.execute(\"ALTER TABLE matches ADD COLUMN game_server_pod text;\")\n\n        cur.execute(\n            \"\"\"\n            SELECT column_name\n            FROM information_schema.columns\n            WHERE table_name='matches' AND column_name='ended_at';\n            \"\"\"\n        )\n        if not cur.fetchone():\n            cur.execute(\"ALTER TABLE matches ADD COLUMN ended_at timestamptz;\")\n\n\ndef track_session_in_redis(session_id: str, game_server_pod: str, connect_host: str, connect_port: int) -> None:\n    redis_client = get_redis_client()\n    if not redis_client:\n        return\n    try:\n        redis_client.sadd(\"active_sessions\", session_id)\n        redis_client.set(f\"session:{session_id}:pod\", game_server_pod, ex=3600)\n        redis_client.set(f\"session:{session_id}:host\", connect_host, ex=3600)\n        redis_client.set(f\"session:{session_id}:port\", str(connect_port), ex=3600)\n    except Exception:  # noqa: BLE001\n        pass\n\n\ndef untrack_session_in_redis(session_id: str) -> None:\n    redis_client = get_redis_client()\n    if not redis_client:\n        return\n    try:\n        redis_client.srem(\"active_sessions\", session_id)\n        redis_client.delete(f\"session:{session_id}:pod\")\n        redis_client.delete(f\"session:{session_id}:host\")\n        redis_client.delete(f\"session:{session_id}:port\")\n        logger.info(f\"Removed active session {session_id} from Redis\")\n    except Exception as e:  # noqa: BLE001\n        logger.warning(f\"Failed to remove session from Redis: {e}\")\n\n\ndef get_player_queue_ts_key(player_id: str) -> str:\n    return f\"matchmaking:queued_at:{player_id}\"\n\n\ndef dequeue_players(redis_client, queue_key: str, count: int) -> List[str]:\n    pipe = redis_client.pipeline()\n    for _ in range(count):\n        pipe.lpop(queue_key)\n    players_list = pipe.execute()\n    players = [p for p in players_list if p]\n    if players:\n        try:\n            cleanup = redis_client.pipeline()\n            for p in players:\n                cleanup.delete(get_player_queue_ts_key(p))\n            cleanup.execute()\n        except Exception:  # noqa: BLE001\n            pass\n    return players\n\n\ndef append_local_queue(player_id: str) -> None:\n    _local_queue.append((player_id, time.time()))\n\n\ndef local_queue_len() -> int:\n    return len(_local_queue)\n\n\ndef local_oldest_wait_seconds() -> float:\n    if not _local_queue:\n        return 0.0\n    return time.time() - _local_queue[0][1]\n\n\ndef local_dequeue(count: int) -> List[str]:\n    players: List[str] = []\n    for _ in range(count):\n        p, _queued_at = _local_queue.popleft()\n        players.append(p)\n    return players\n",
  "src/app/backend/k8s_game_server.py": "import json\nimport logging\nimport os\nimport time\nfrom typing import List\n\nfrom kubernetes import client, config\nfrom kubernetes.client.rest import ApiException\n\nfrom settings import NAMESPACE\n\nlogger = logging.getLogger(__name__)\n\n_k8s_api = None\n\n\ndef _load_k8s_config() -> None:\n    try:\n        config.load_incluster_config()\n    except Exception:  # noqa: BLE001\n        config.load_kube_config()\n\n\ndef get_k8s_api():\n    global _k8s_api\n    if _k8s_api is None:\n        _load_k8s_config()\n        _k8s_api = client.AppsV1Api()\n    return _k8s_api\n\n\ndef get_core_v1_api():\n    if not hasattr(get_core_v1_api, \"_api\"):\n        _load_k8s_config()\n        get_core_v1_api._api = client.CoreV1Api()\n    return get_core_v1_api._api\n\n\ndef wait_for_game_server_ready(session_id: str, timeout_seconds: float = 45.0) -> bool:\n    core_api = get_core_v1_api()\n    label_selector = f\"app=game-server,session_id={session_id}\"\n    deadline = time.time() + timeout_seconds\n\n    while time.time() < deadline:\n        try:\n            pods = core_api.list_namespaced_pod(\n                namespace=NAMESPACE,\n                label_selector=label_selector,\n            )\n            for pod in pods.items:\n                status = pod.status\n                if status is None or status.phase != \"Running\":\n                    continue\n                if any(cs.ready for cs in (status.container_statuses or [])):\n                    return True\n        except Exception as e:  # noqa: BLE001\n            logger.warning(f\"Error while waiting for game server readiness ({session_id}): {e}\")\n        time.sleep(0.5)\n    return False\n\n\ndef create_game_server_pod(session_id: str, players: List[str]) -> tuple[str, str, int]:\n    k8s_apps_api = get_k8s_api()\n    core_api = get_core_v1_api()\n\n    pod_name = f\"game-server-{session_id[:8]}\"\n    logger.info(f\"Creating game server pod {pod_name} in namespace {NAMESPACE}\")\n\n    deployment = client.V1Deployment(\n        metadata=client.V1ObjectMeta(\n            name=pod_name,\n            namespace=NAMESPACE,\n            labels={\"app\": \"game-server\", \"session_id\": session_id},\n        ),\n        spec=client.V1DeploymentSpec(\n            replicas=1,\n            selector=client.V1LabelSelector(\n                match_labels={\"app\": \"game-server\", \"session_id\": session_id}\n            ),\n            template=client.V1PodTemplateSpec(\n                metadata=client.V1ObjectMeta(\n                    labels={\"app\": \"game-server\", \"session_id\": session_id}\n                ),\n                spec=client.V1PodSpec(\n                    containers=[\n                        client.V1Container(\n                            name=\"game-server\",\n                            image=\"game-server:local\",\n                            image_pull_policy=\"IfNotPresent\",\n                            ports=[client.V1ContainerPort(container_port=8080)],\n                            env=[\n                                client.V1EnvVar(name=\"SESSION_ID\", value=session_id),\n                                client.V1EnvVar(name=\"PLAYERS\", value=json.dumps(players)),\n                                client.V1EnvVar(name=\"PORT\", value=\"8080\"),\n                            ],\n                        )\n                    ],\n                    restart_policy=\"Always\",\n                ),\n            ),\n        ),\n    )\n\n    try:\n        k8s_apps_api.create_namespaced_deployment(namespace=NAMESPACE, body=deployment)\n        logger.info(f\"Successfully created deployment {pod_name}\")\n    except ApiException as e:\n        logger.error(f\"Failed to create deployment: status={e.status}, reason={e.reason}, body={e.body}\")\n        if e.status != 409:\n            raise\n\n    svc = client.V1Service(\n        metadata=client.V1ObjectMeta(\n            name=pod_name,\n            namespace=NAMESPACE,\n            labels={\"app\": \"game-server\", \"session_id\": session_id},\n        ),\n        spec=client.V1ServiceSpec(\n            type=\"NodePort\",\n            selector={\"app\": \"game-server\", \"session_id\": session_id},\n            ports=[client.V1ServicePort(port=8080, target_port=8080, protocol=\"TCP\")],\n        ),\n    )\n    try:\n        core_api.create_namespaced_service(namespace=NAMESPACE, body=svc)\n        logger.info(f\"Created Service {pod_name} (NodePort)\")\n    except ApiException as e:\n        if e.status != 409:\n            logger.warning(f\"Failed to create Service for {pod_name}: {e}\")\n            return pod_name, \"\", 0\n\n    time.sleep(0.5)\n    try:\n        created = core_api.read_namespaced_service(name=pod_name, namespace=NAMESPACE)\n        node_port = created.spec.ports[0].node_port if created.spec.ports else 0\n    except Exception:  # noqa: BLE001\n        node_port = 0\n\n    connect_host = os.getenv(\"GAME_SERVER_CONNECT_HOST\", \"\")\n    if not connect_host:\n        try:\n            nodes = core_api.list_node()\n            for node in nodes.items:\n                for addr in node.status.addresses or []:\n                    if addr.type in (\"ExternalIP\", \"InternalIP\"):\n                        connect_host = addr.address\n                        break\n                if connect_host:\n                    break\n        except Exception as e:  # noqa: BLE001\n            logger.warning(f\"Failed to auto-detect node IP for game server connect_host: {e}\")\n    if not connect_host:\n        connect_host = \"localhost\"\n        logger.warning(\n            \"Falling back to localhost for game-server connect_host; this may be unreachable for NodePort clients\"\n        )\n\n    if not wait_for_game_server_ready(session_id, timeout_seconds=45.0):\n        logger.warning(f\"Game server {pod_name} not ready within timeout; clients may need to retry connect\")\n\n    return pod_name, connect_host, node_port or 0\n\n\ndef delete_game_server_pod(session_id: str) -> None:\n    k8s_apps_api = get_k8s_api()\n    core_api = get_core_v1_api()\n    pod_name = f\"game-server-{session_id[:8]}\"\n\n    try:\n        core_api.delete_namespaced_service(name=pod_name, namespace=NAMESPACE)\n        logger.info(f\"Deleted Service {pod_name}\")\n    except ApiException as e:\n        if e.status != 404:\n            logger.warning(f\"Failed to delete Service {pod_name}: {e}\")\n\n    max_retries = 3\n    for attempt in range(max_retries):\n        try:\n            k8s_apps_api.delete_namespaced_deployment(\n                name=pod_name,\n                namespace=NAMESPACE,\n                body=client.V1DeleteOptions(propagation_policy=\"Foreground\"),\n            )\n            logger.info(f\"Successfully deleted game server pod {pod_name}\")\n            return\n        except ApiException as e:\n            if e.status == 404:\n                logger.info(f\"Game server pod {pod_name} already deleted\")\n                return\n            if attempt < max_retries - 1:\n                wait_time = 2 ** attempt\n                logger.warning(f\"Failed to delete {pod_name} (attempt {attempt + 1}/{max_retries}): {e}, retrying in {wait_time}s\")\n                time.sleep(wait_time)\n            else:\n                logger.error(f\"Failed to delete {pod_name} after {max_retries} attempts: {e}\")\n                raise\n",
  "src/app/backend/Dockerfile": "FROM python:3.12-slim\n\nWORKDIR /app\n\nRUN pip install --no-cache-dir fastapi uvicorn[standard] psycopg2-binary kubernetes redis\n\nCOPY *.py .\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n\n",
  "src/app/load/main.py": "import argparse\nimport os\nimport time\n\nfrom lifecycle import simulate_client_lifecycle\n\n\ndef main() -> None:\n    \"\"\"\n    Realistic game client load simulator.\n\n    Usage:\n      python main.py 100\n\n    Simulates clients that:\n    - Join matchmaking\n    - Start matches (creates game server pods)\n    - Play for a duration (with reconnection checks)\n    - End matches (deletes game server pods)\n    \"\"\"\n    default_url = os.getenv(\"TARGET_URL\", \"http://localhost:8080\")\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"clients\",\n        type=int,\n        help=\"number of concurrent client lifecycles\",\n    )\n    parser.add_argument(\n        \"--url\",\n        default=default_url,\n        help=\"base URL (defaults to localhost on port 8080)\",\n    )\n    parser.add_argument(\n        \"--match-duration\",\n        type=float,\n        default=float(os.getenv(\"MATCH_DURATION\", \"30.0\")),\n        help=\"seconds each match lasts\",\n    )\n    parser.add_argument(\n        \"--spawn-rate\",\n        type=float,\n        default=float(os.getenv(\"SPAWN_RATE\", \"0\")),  # 0 = spawn all immediately\n        help=\"clients spawned per second (0 = spawn all immediately)\",\n    )\n    args = parser.parse_args()\n\n    gs_host_override = os.getenv(\"GAME_SERVER_HOST_OVERRIDE\", \"\").strip()\n    if gs_host_override:\n        print(f\"Overriding game server host with: {gs_host_override}\")\n    if args.spawn_rate > 0:\n        print(\n            f\"Spawning {args.clients} clients at {args.spawn_rate}/s \"\n            f\"(match duration: {args.match_duration}s, Ctrl+C to stop)\"\n        )\n    else:\n        print(\n            f\"Spawning all {args.clients} clients simultaneously \"\n            f\"(match duration: {args.match_duration}s, Ctrl+C to stop)\"\n        )\n\n    import threading\n\n    active_threads = []\n    \n    try:\n        # Spawn all clients\n        for i in range(args.clients):\n            thread = threading.Thread(\n                target=simulate_client_lifecycle,\n                args=(args.url, args.match_duration),\n                daemon=True,\n            )\n            thread.start()\n            active_threads.append(thread)\n            \n            # Spawn rate limiting (if specified)\n            if args.spawn_rate > 0 and i < args.clients - 1:\n                time.sleep(1.0 / args.spawn_rate)\n        \n        # Wait for all threads\n        for thread in active_threads:\n            thread.join(timeout=args.match_duration + 30)\n        \n    except KeyboardInterrupt:\n        print(\"\\nStopping clients...\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "src/app/load/client.py": "import os\nimport socket\nimport time\n\nimport requests\n\n\nclass GameClient:\n    \"\"\"Simulates a realistic game client: join queue -> get server address -> play -> end.\"\"\"\n\n    def __init__(self, base_url: str, player_id: str):\n        self.base_url = base_url\n        self.player_id = player_id\n        self.game_server_host_override = os.getenv(\"GAME_SERVER_HOST_OVERRIDE\", \"\").strip()\n        self.session_id = None\n        self.connect_host = None\n        self.connect_port = None\n\n    def _log(self, msg: str) -> None:\n        print(f\"  {self.player_id[:8]} {msg}\")\n\n    def join_matchmaking(self) -> dict | None:\n        self._log(\"state=joining_matchmaking\")\n        try:\n            resp = requests.post(\n                f\"{self.base_url}/api/match/join\",\n                json={\"player_id\": self.player_id},\n                timeout=15.0,\n            )\n            resp.raise_for_status()\n            data = resp.json()\n            self.session_id = data.get(\"session_id\")\n            self.connect_host = self.game_server_host_override or data.get(\"connect_host\") or None\n            self.connect_port = data.get(\"connect_port\") or 0\n            if self.connect_host and self.connect_port:\n                self._log(\n                    f\"state=matched session={self.session_id} server={self.connect_host}:{self.connect_port}\"\n                )\n            else:\n                self._log(f\"state=queued session={self.session_id}\")\n            return data\n        except Exception as e:  # noqa: BLE001\n            self._log(f\"state=join_error error={e}\")\n            return None\n\n    def poll_status(self) -> dict | None:\n        try:\n            resp = requests.get(\n                f\"{self.base_url}/api/match/status\",\n                params={\"player_id\": self.player_id},\n                timeout=5.0,\n            )\n            resp.raise_for_status()\n            data = resp.json()\n            if data.get(\"status\") == \"matched\":\n                self.session_id = data.get(\"session_id\")\n                self.connect_host = self.game_server_host_override or data.get(\"connect_host\")\n                self.connect_port = data.get(\"connect_port\", 0)\n            return data\n        except Exception as e:  # noqa: BLE001\n            self._log(f\"state=status_error error={e}\")\n            return None\n\n    def end_match(self) -> bool:\n        if not self.session_id or self.session_id.startswith(\"pending\"):\n            return False\n        try:\n            resp = requests.post(\n                f\"{self.base_url}/api/match/{self.session_id}/end\",\n                timeout=5.0,\n            )\n            resp.raise_for_status()\n            self._log(f\"state=session_ended session={self.session_id}\")\n            return True\n        except Exception as e:  # noqa: BLE001\n            self._log(f\"state=end_error session={self.session_id} error={e}\")\n            return False\n\n    def connect_and_wait_for_stop(\n        self,\n        *,\n        match_duration_seconds: int = 30,\n        recv_timeout: float = 60.0,\n        connect_retries: int = 60,\n        connect_retry_delay: float = 1.0,\n    ) -> float | None:\n        if not self.connect_host or not self.connect_port:\n            return None\n        running_length = None\n        sock = None\n        last_server_state = None\n        self._log(f\"state=tcp_connecting target={self.connect_host}:{self.connect_port}\")\n        for attempt in range(connect_retries):\n            try:\n                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                sock.settimeout(recv_timeout)\n                sock.connect((self.connect_host, self.connect_port))\n                self._log(\n                    f\"state=tcp_connected target={self.connect_host}:{self.connect_port} attempts={attempt + 1}\"\n                )\n                break\n            except (ConnectionRefusedError, OSError) as e:\n                if sock is not None:\n                    try:\n                        sock.close()\n                    except Exception:  # noqa: BLE001\n                        pass\n                    sock = None\n                if attempt < connect_retries - 1:\n                    time.sleep(connect_retry_delay)\n                else:\n                    self._log(\n                        f\"state=tcp_connect_failed attempts={connect_retries} error={e}\"\n                    )\n                    return None\n        if sock is None:\n            return None\n        try:\n            sock.sendall(f\"REQUEST_MATCH {match_duration_seconds}\\n\".encode())\n            first_payload = sock.recv(512).decode()\n            for line in first_payload.splitlines():\n                line = line.strip()\n                if line.startswith(\"RUNNING_LENGTH \"):\n                    token = line.split(maxsplit=1)[1].split()[0]\n                    running_length = float(token)\n                    self._log(f\"state=running_length value={running_length}\")\n                if line.startswith(\"STATE \"):\n                    server_state = line.split(maxsplit=1)[1].strip().lower()\n                    if server_state != last_server_state:\n                        last_server_state = server_state\n                        self._log(f\"state=server_{server_state}\")\n            while True:\n                data = sock.recv(256).decode()\n                if not data:\n                    self._log(\"state=tcp_disconnected\")\n                    break\n                for line in data.splitlines():\n                    line = line.strip()\n                    if line.startswith(\"STATE \"):\n                        server_state = line.split(maxsplit=1)[1].strip().lower()\n                        if server_state != last_server_state:\n                            last_server_state = server_state\n                            self._log(f\"state=server_{server_state}\")\n                        if server_state == \"stop\":\n                            return running_length\n        except (socket.timeout, ConnectionResetError, BrokenPipeError, OSError) as e:\n            if \"STATE STOP\" not in str(e).upper():\n                self._log(f\"state=tcp_error error={e}\")\n        finally:\n            try:\n                sock.close()\n            except Exception:  # noqa: BLE001\n                pass\n        return running_length\n",
  "src/app/load/lifecycle.py": "import time\nimport uuid\n\nfrom client import GameClient\n\n\ndef simulate_client_lifecycle(base_url: str, match_duration: float) -> None:\n    \"\"\"\n    Flow: join queue at proxy -> backend allocates game server when ready ->\n    proxy returns address (in join response or via status poll) -> play -> end.\n    \"\"\"\n    player_id = str(uuid.uuid4())\n    client = GameClient(base_url, player_id)\n\n    result = client.join_matchmaking()\n    if not result:\n        return\n\n    session_id = result.get(\"session_id\", \"\")\n    if result.get(\"connect_host\") and result.get(\"connect_port\"):\n        pass\n    elif session_id.startswith(\"pending\"):\n        max_wait = 60.0\n        wait_start = time.time()\n        last_status = None\n        while time.time() - wait_start < max_wait:\n            status = client.poll_status()\n            if status:\n                current = status.get(\"status\")\n                if current != last_status:\n                    last_status = current\n                    client._log(f\"state=match_status_{current}\")\n            if status and status.get(\"status\") == \"matched\":\n                client._log(\n                    f\"state=matched session={client.session_id} server={client.connect_host}:{client.connect_port}\"\n                )\n                break\n            if status and status.get(\"status\") == \"ended\":\n                client._log(\"state=session_ended_before_connect\")\n                return\n            time.sleep(0.5)\n        if not client.session_id or not client.connect_host:\n            client._log(\"state=match_timeout_waiting_for_server\")\n            return\n    else:\n        if not client.connect_host:\n            return\n\n    client.connect_and_wait_for_stop(match_duration_seconds=30, recv_timeout=max(60.0, 30 + 10))\n    client._log(f\"state=ending_session session={client.session_id}\")\n    client.end_match()\n",
  "src/databases/postgres.yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  namespace: databases\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n        - name: postgres\n          image: postgres:16-alpine\n          ports:\n            - containerPort: 5432\n          env:\n            - name: POSTGRES_DB\n              value: app\n            - name: POSTGRES_USER\n              value: postgres\n            - name: POSTGRES_PASSWORD\n              value: postgres\n          readinessProbe:\n            exec:\n              command: [\"sh\", \"-c\", \"pg_isready -U postgres -d app\"]\n            initialDelaySeconds: 5\n            periodSeconds: 5\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres\n  namespace: databases\nspec:\n  selector:\n    app: postgres\n  ports:\n    - port: 5432\n      targetPort: 5432\n\n",
  "src/databases/redis.yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  namespace: databases\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n        - name: redis\n          image: redis:7-alpine\n          ports:\n            - containerPort: 6379\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis\n  namespace: databases\nspec:\n  selector:\n    app: redis\n  ports:\n    - port: 6379\n      targetPort: 6379\n",
  "src/databases/namespace.yaml": "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: databases\n\n",
  "src/managing/portainer-values.yaml": "service:\n  type: ClusterIP\n  httpPort: 9000\n\n# Manage the cluster Portainer runs in (in-cluster)\nlocalMgmt: true\n",
  "src/k8s/base/cleanup-job.yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: game-server-cleanup\nspec:\n  schedule: \"*/5 * * * *\"  # Every 5 minutes\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          serviceAccountName: game-backend\n          containers:\n            - name: cleanup\n              image: bitnami/kubectl:latest\n              command:\n                - /bin/sh\n                - -c\n                - |\n                  # Find game server deployments for ended matches\n                  kubectl get deployments -l app=game-server -o json | \\\n                  jq -r '.items[] | select(.metadata.name | startswith(\"game-server-\")) | .metadata.name' | \\\n                  while read dep; do\n                    session_id=$(echo $dep | sed 's/game-server-//')\n                    # Check if match is ended in DB (via backend service)\n                    if kubectl exec deployment/game-backend -- python3 -c \"\n                      import psycopg2, os\n                      conn = psycopg2.connect('postgresql://postgres:postgres@postgres.databases.svc.cluster.local:5432/app')\n                      cur = conn.cursor()\n                      cur.execute('SELECT ended_at FROM matches WHERE session_id LIKE %s', (f'%{session_id}%',))\n                      row = cur.fetchone()\n                      if row and row[0]:\n                        exit(0)  # Match ended\n                      exit(1)  # Match still active\n                    \" 2>/dev/null; then\n                      echo \"Deleting orphaned deployment: $dep\"\n                      kubectl delete deployment $dep --ignore-not-found\n                    fi\n                  done\n          restartPolicy: OnFailure\n",
  "src/k8s/base/backend-hpa.yaml": "apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: game-backend-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: game-backend\n  minReplicas: 2\n  maxReplicas: 20  # Increased max for faster scaling\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 50  # Lower threshold = scale sooner\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 0  # No delay on scale down\n      policies:\n      - type: Pods\n        value: 5  # Remove 5 pods at once\n        periodSeconds: 15\n    scaleUp:\n      stabilizationWindowSeconds: 0  # No delay\n      policies:\n      - type: Percent\n        value: 500  # 5x increase per evaluation\n        periodSeconds: 15\n      - type: Pods\n        value: 10  # Add 10 pods at once if needed\n        periodSeconds: 15\n      selectPolicy: Max\n\n",
  "src/k8s/base/backend.yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: game-backend\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: game-backend\n  template:\n    metadata:\n      labels:\n        app: game-backend\n    spec:\n      serviceAccountName: game-backend\n      containers:\n        - name: backend\n          image: game-backend:local\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8080\n          env:\n            - name: PYTHONUNBUFFERED\n              value: \"1\"\n            - name: DATABASE_URL\n              value: \"postgresql://postgres:postgres@postgres.databases.svc.cluster.local:5432/app\"\n            - name: REDIS_HOST\n              value: \"redis.databases.svc.cluster.local\"\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: SESSION_SIZE\n              value: \"12\"\n            - name: FLUSH_WAIT_SECONDS\n              value: \"15\"\n            - name: MIN_PARTIAL_SESSION_SIZE\n              value: \"2\"\n            - name: NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            # Optional override for returned game server host.\n            # Leave unset for kind so backend auto-detects the node InternalIP\n            # (e.g. 172.18.0.2), which is reachable for NodePort TCP.\n            # - name: GAME_SERVER_CONNECT_HOST\n            #   value: \"172.18.0.2\"\n          resources:\n            requests:\n              cpu: 100m\n              memory: 128Mi\n            limits:\n              cpu: 500m\n              memory: 256Mi\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 0  # Start checking immediately\n            periodSeconds: 1  # Check every second\n            timeoutSeconds: 1\n            failureThreshold: 2  # Fail fast\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 3  # Give it 3s to start\n            periodSeconds: 2\n            timeoutSeconds: 1\n            failureThreshold: 2\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: game-backend\nspec:\n  selector:\n    app: game-backend\n  ports:\n    - port: 8080\n      targetPort: 8080\n\n",
  "src/k8s/base/backend-rbac.yaml": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: game-backend\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: game-backend-pod-manager\nrules:\n  - apiGroups: [\"apps\"]\n    resources: [\"deployments\"]\n    verbs: [\"create\", \"delete\", \"get\", \"list\"]\n  - apiGroups: [\"\"]\n    resources: [\"pods\"]\n    verbs: [\"get\", \"list\"]\n  - apiGroups: [\"\"]\n    resources: [\"services\"]\n    verbs: [\"create\", \"delete\", \"get\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: game-backend-pod-manager\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: game-backend-pod-manager\nsubjects:\n  - kind: ServiceAccount\n    name: game-backend\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: game-backend-node-reader\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"get\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: game-backend-node-reader\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: game-backend-node-reader\nsubjects:\n  - kind: ServiceAccount\n    name: game-backend\n    namespace: default\n",
  "src/k8s/base/proxy-hpa.yaml": "apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: game-proxy-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: game-proxy\n  minReplicas: 2\n  maxReplicas: 20  # Increased max for faster scaling\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 50  # Lower threshold = scale sooner\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 0  # No delay on scale down\n      policies:\n      - type: Pods\n        value: 5  # Remove 5 pods at once\n        periodSeconds: 15\n    scaleUp:\n      stabilizationWindowSeconds: 0  # No delay\n      policies:\n      - type: Percent\n        value: 500  # 5x increase per evaluation\n        periodSeconds: 15\n      - type: Pods\n        value: 10  # Add 10 pods at once if needed\n        periodSeconds: 15\n      selectPolicy: Max\n\n",
  "src/k8s/base/proxy.yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: game-proxy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: game-proxy\n  template:\n    metadata:\n      labels:\n        app: game-proxy\n    spec:\n      containers:\n        - name: proxy\n          image: game-proxy:local\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8080\n          env:\n            - name: BACKEND_URL\n              value: \"http://game-backend:8080\"\n            - name: PYTHONUNBUFFERED\n              value: \"1\"\n          resources:\n            requests:\n              cpu: 100m\n              memory: 128Mi\n            limits:\n              cpu: 1000m\n              memory: 512Mi\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 0  # Start checking immediately\n            periodSeconds: 1  # Check every second\n            timeoutSeconds: 1\n            failureThreshold: 2  # Fail fast\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 3  # Give it 3s to start\n            periodSeconds: 2\n            timeoutSeconds: 1\n            failureThreshold: 2\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: game-proxy\nspec:\n  selector:\n    app: game-proxy\n  ports:\n    - port: 8080\n      targetPort: 8080\n\n",
  "src/monitoring/values.yaml": "grafana:\n  enabled: true\n  service:\n    type: ClusterIP\n\nprometheus:\n  service:\n    type: ClusterIP\n\nalertmanager:\n  enabled: false\n",
  "src/scripts/ping-redis.sh": "#!/usr/bin/env bash\nset -euo pipefail\necho \"Pinging Redis at localhost:6379...\"\nredis-cli -h localhost -p 6379 ping || true\n",
  "src/scripts/ping-postgres.sh": "#!/usr/bin/env bash\nset -euo pipefail\n\necho \"Pinging Postgres at localhost:5432...\"\npg_isready -h localhost -p 5432 -U postgres -d app || true\n\n",
  "src/scripts/stop-port-forward.sh": "#!/usr/bin/env bash\nset -euo pipefail\n\nPID_DIR=.pids\n[[ -d \"${PID_DIR}\" ]] || exit 0\n\nfor pidfile in \"${PID_DIR}\"/*.pid; do\n  [[ -f \"${pidfile}\" ]] || continue\n  pid=$(cat \"${pidfile}\")\n  kill \"${pid}\" 2>/dev/null || true\ndone\n\nrm -rf \"${PID_DIR}\"\n",
  "src/scripts/free-local-db-ports.sh": "#!/usr/bin/env bash\nset -euo pipefail\n\nstop_service_if_active() {\n  local service_name=$1\n  if ! command -v systemctl >/dev/null 2>&1; then\n    echo \"systemctl not found; cannot stop ${service_name} automatically.\"\n    return\n  fi\n\n  if systemctl is-active --quiet \"${service_name}\"; then\n    echo \"Stopping ${service_name}...\"\n    if sudo -n systemctl stop \"${service_name}\" >/dev/null 2>&1; then\n      echo \"Stopped ${service_name}.\"\n      return\n    fi\n\n    echo \"sudo password may be required to stop ${service_name}.\"\n    if sudo systemctl stop \"${service_name}\"; then\n      echo \"Stopped ${service_name}.\"\n    else\n      echo \"Failed to stop ${service_name}.\"\n    fi\n  else\n    echo \"${service_name} is not active.\"\n  fi\n}\n\ncheck_and_free_port() {\n  local port=$1\n  local expected_regex=$2\n  local service_name=$3\n  local label=$4\n\n  local line\n  line=\"$(ss -ltnp 2>/dev/null | awk -v p=\":${port}\" '$4 ~ p\"$\" {print; exit}')\"\n\n  if [[ -z \"${line}\" ]]; then\n    echo \"${label} port ${port} is already free.\"\n    return\n  fi\n\n  echo \"${label} port ${port} is occupied: ${line}\"\n\n  if [[ \"${line}\" =~ ${expected_regex} ]]; then\n    stop_service_if_active \"${service_name}\"\n  else\n    echo \"Port ${port} is not occupied by host ${label}; leaving it unchanged.\"\n  fi\n}\n\nis_service_active() {\n  local service_name=$1\n  if ! command -v systemctl >/dev/null 2>&1; then\n    return 1\n  fi\n  systemctl is-active --quiet \"${service_name}\"\n}\n\nredis_is_responding() {\n  command -v redis-cli >/dev/null 2>&1 \\\n    && [[ \"$(redis-cli -h 127.0.0.1 -p 6379 PING 2>/dev/null || true)\" == \"PONG\" ]]\n}\n\npostgres_is_responding() {\n  command -v pg_isready >/dev/null 2>&1 \\\n    && pg_isready -h 127.0.0.1 -p 5432 >/dev/null 2>&1\n}\n\n# Prefer process-name check. If process names are hidden on this host, fall back\n# to checking whether local Redis/Postgres services are active and responding.\ncheck_and_free_port 6379 \"redis-server\" \"redis-server\" \"Redis\"\nif is_service_active \"redis-server\" && redis_is_responding; then\n  stop_service_if_active \"redis-server\"\nfi\n\ncheck_and_free_port 5432 \"(postgres|postmaster)\" \"postgresql\" \"Postgres\"\nif is_service_active \"postgresql\" && postgres_is_responding; then\n  stop_service_if_active \"postgresql\"\nfi\n\necho \"Done. You can now run your preferred kubectl port-forward commands.\"\n",
  "src/scripts/clear-databases.sh": "#!/usr/bin/env bash\nset -euo pipefail\n\necho \"Clearing Redis...\"\nkubectl exec deployment/redis -n databases -- redis-cli FLUSHALL || echo \"Redis flush failed\"\n\necho \"Clearing Postgres...\"\nif kubectl exec deployment/postgres -n databases -- psql -U postgres -d app -tAc \"SELECT to_regclass('public.matches') IS NOT NULL;\" | grep -q \"t\"; then\n  kubectl exec deployment/postgres -n databases -- psql -U postgres -d app -c \"TRUNCATE TABLE matches CASCADE;\" || echo \"Postgres truncate failed\"\nelse\n  echo \"matches table does not exist yet, skipping truncate\"\nfi\n\nkubectl exec deployment/postgres -n databases -- psql -U postgres -d app -c \"DROP TABLE IF EXISTS match_events, game_server_stats;\" || echo \"Postgres telemetry table cleanup failed\"\n\necho \"Cleaning up orphaned game server pods and services...\"\nkubectl delete deployment -l app=game-server --ignore-not-found || true\nkubectl delete service -l app=game-server --ignore-not-found || true\n\necho \"Databases cleared!\"\n",
  "src/scripts/port-forward.sh": "#!/usr/bin/env bash\nset -euo pipefail\n\nPID_DIR=.pids\nmkdir -p \"${PID_DIR}\"\n\nstart_pf () {\n  local name=$1\n  local namespace=$2\n  local svc=$3\n  local local_port=$4\n  local remote_port=$5\n\n  if [[ -f \"${PID_DIR}/${name}.pid\" ]] && kill -0 \"$(cat \"${PID_DIR}/${name}.pid\")\" 2>/dev/null; then\n    return\n  fi\n\n  kubectl port-forward -n \"${namespace}\" \"svc/${svc}\" \\\n    \"${local_port}:${remote_port}\" \\\n    > /dev/null 2>&1 &\n\n  echo $! > \"${PID_DIR}/${name}.pid\"\n}\n\n# Monitoring\nstart_pf grafana     monitoring monitoring-grafana                     3000 80\nstart_pf prometheus  monitoring monitoring-kube-prometheus-prometheus  9090 9090\n\n# Managing (Portainer)\nstart_pf portainer   portainer  portainer                             9000 9000\n\n# Databases\nstart_pf redis       databases  redis                                6379 6379\nstart_pf postgres    databases  postgres                             5432 5432\n\n# Stateful TCP demo (optional: run make tcp-demo first)\nif kubectl get svc tcp-demo -n tcp-demo &>/dev/null; then\n  start_pf tcp-demo      tcp-demo tcp-demo 7654 7654\n  start_pf tcp-demo-web tcp-demo tcp-demo 8081 8080\nfi\n\necho \"\"\necho \"kubectl get secret --namespace monitoring -l app.kubernetes.io/component=admin-secret -o jsonpath=\"{.items[0].data.admin-password}\" | base64 --decode ; echo\"\necho \"READY:\"\necho \"  Grafana    http://localhost:3000\"\necho \"  Prometheus http://localhost:9090\"\necho \"  Portainer  http://localhost:9000 (create admin on first visit)\"\necho \"  Redis      localhost:6379\"\necho \"  Postgres   localhost:5432 (postgres/postgres, db=app)\"\necho \"  TCP demo   localhost:7654 (TCP) | http://localhost:8081 (web UI; run make tcp-demo)\"",
  "src/scripts/monitors.sh": "#!/usr/bin/env bash\nset -euo pipefail\n\necho \"Installing Prometheus + Grafana (kube-prometheus-stack)...\"\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts >/dev/null 2>&1 || true\nhelm repo update >/dev/null\n\nhelm upgrade --install monitoring prometheus-community/kube-prometheus-stack \\\n  --namespace monitoring \\\n  --create-namespace \\\n  --values src/monitoring/values.yaml\n\nkubectl rollout status deployment/monitoring-grafana -n monitoring\n",
  "src/scripts/teardown.sh": "#!/usr/bin/env bash\nset -euo pipefail\n\n./src/scripts/stop-port-forward.sh || true\nkind delete cluster --name dev\n",
  "src/scripts/cluster.sh": "#!/usr/bin/env bash\nset -euo pipefail\n\nCLUSTER_NAME=dev\n\ncommand -v kind >/dev/null\ncommand -v kubectl >/dev/null\ncommand -v helm >/dev/null\n\necho \"Creating kind cluster...\"\nkind create cluster \\\n  --name \"${CLUSTER_NAME}\" \\\n  --config src/kind/cluster.yaml || true\n\necho \"Installing metrics-server for HPA...\"\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\nkubectl patch deployment metrics-server -n kube-system --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/-\", \"value\": \"--kubelet-insecure-tls\"}]' || true\nkubectl rollout status deployment/metrics-server -n kube-system --timeout=60s || true\n",
  "src/scripts/managers.sh": "#!/usr/bin/env bash\nset -euo pipefail\n\necho \"Installing Portainer (managing UI)...\"\nhelm repo add portainer https://portainer.github.io/k8s/ >/dev/null 2>&1 || true\nhelm repo update >/dev/null\nhelm upgrade --install portainer portainer/portainer \\\n  --namespace portainer \\\n  --create-namespace \\\n  --values src/managing/portainer-values.yaml\nkubectl rollout status deployment/portainer -n portainer\n"
}